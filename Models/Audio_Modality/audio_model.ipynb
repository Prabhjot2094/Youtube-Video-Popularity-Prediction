{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from keras.utils import np_utils\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, Dense, Embedding, SimpleRNN, LSTM\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(961, 800, 25)\n"
     ]
    }
   ],
   "source": [
    "audio_folder = '/Users/lindawong/Desktop/github/Youtube-Video-Popularity-Prediction/processed_audio_0'\n",
    "audio_df = pd.read_csv('/Users/lindawong/Desktop/github/Youtube-Video-Popularity-Prediction/audio_analysis/rating.csv')\n",
    "\n",
    "audio_ids = pd.read_json('/Users/lindawong/Desktop/github/Youtube-Video-Popularity-Prediction/Models/Audio Modality/video_ids_balanced.json')\n",
    "audio_ids = audio_ids.to_numpy().squeeze()\n",
    "audio_ids = [aid[:-4] for aid in audio_ids]\n",
    "\n",
    "audio_df = audio_df[audio_df['video_id'].isin(audio_ids)]\n",
    "audio_ratings = audio_df['rating'].to_numpy()\n",
    "\n",
    "num_audio = len(audio_ids)\n",
    "max_frames = 800\n",
    "num_features = 25\n",
    "\n",
    "# construct audio data matrix of shape (number of audio clips, number of frames, number of features)\n",
    "audio_data = np.zeros((num_audio, max_frames, num_features))\n",
    "for i, aid in enumerate(audio_ids):\n",
    "    file = 'audio_' + aid + '.csv'\n",
    "    data = pd.read_csv(os.path.join(audio_folder, file), index_col=0).to_numpy()\n",
    "    \n",
    "    num_frames = data.shape[0]\n",
    "    if data.shape[0] < max_frames:\n",
    "        audio_data[i,:num_frames,:] = data\n",
    "    else:\n",
    "        audio_data[i,:,:] = data[:800,:]\n",
    "        \n",
    "print(audio_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-0.002, 0.1]    100\n",
      "(0.1, 0.2]       100\n",
      "(0.2, 0.3]       100\n",
      "(0.3, 0.4]       100\n",
      "(0.4, 0.5]       100\n",
      "(0.5, 0.6]       100\n",
      "(0.6, 0.7]       100\n",
      "(0.7, 0.8]        98\n",
      "(0.8, 0.9]        74\n",
      "(0.9, 1.0]        89\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1395557d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# analyze distribution of ratings\n",
    "rating = pd.Series(audio_ratings)\n",
    "print(rating.value_counts(bins=10).sort_index())\n",
    "rating.hist(bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nbalanced_df = pd.DataFrame(columns=audio_df.columns)\\nfor j in range(10):\\n    temp_df = pd.DataFrame()\\n    lower = j/10\\n    upper = (j+1)/10 if j < 9 else 1.1\\n    temp_data = audio_df[(audio_df['rating'] >= lower) & (audio_df['rating'] < upper)]\\n    temp_data = temp_data.sample(frac=1)\\n    if temp_data.shape[0] > 100:\\n        temp_data = temp_data.head(100)\\n    balanced_df = balanced_df.append(temp_data)\\n\\nb_audio_ratings = balanced_df['rating']\\nrating = pd.Series(b_audio_ratings)\\nprint(rating.value_counts(bins=10).sort_index())\\nrating.hist(bins=10)\\n\\nb_audio_ids = balanced_df['video_id']\\nnum_b_audio = len(b_audio_ids)\\nmax_frames = 800\\nnum_features = 25\\n\\nb_audio_data = np.zeros((num_b_audio, max_frames, num_features))\\nfor i, aid in enumerate(b_audio_ids):\\n    file = 'audio_' + aid + '.csv'\\n    data = pd.read_csv(os.path.join(audio_folder, file), index_col=0).to_numpy()\\n    \\n    num_frames = data.shape[0]\\n    if data.shape[0] < max_frames:\\n        b_audio_data[i,:num_frames,:] = data\\n    else:\\n        b_audio_data[i,:,:] = data[:800,:]\\n        \\nprint(b_audio_data.shape)\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# balance dataset\n",
    "\"\"\"\n",
    "balanced_df = pd.DataFrame(columns=audio_df.columns)\n",
    "for j in range(10):\n",
    "    temp_df = pd.DataFrame()\n",
    "    lower = j/10\n",
    "    upper = (j+1)/10 if j < 9 else 1.1\n",
    "    temp_data = audio_df[(audio_df['rating'] >= lower) & (audio_df['rating'] < upper)]\n",
    "    temp_data = temp_data.sample(frac=1)\n",
    "    if temp_data.shape[0] > 100:\n",
    "        temp_data = temp_data.head(100)\n",
    "    balanced_df = balanced_df.append(temp_data)\n",
    "\n",
    "b_audio_ratings = balanced_df['rating']\n",
    "rating = pd.Series(b_audio_ratings)\n",
    "print(rating.value_counts(bins=10).sort_index())\n",
    "rating.hist(bins=10)\n",
    "\n",
    "b_audio_ids = balanced_df['video_id']\n",
    "num_b_audio = len(b_audio_ids)\n",
    "max_frames = 800\n",
    "num_features = 25\n",
    "\n",
    "b_audio_data = np.zeros((num_b_audio, max_frames, num_features))\n",
    "for i, aid in enumerate(b_audio_ids):\n",
    "    file = 'audio_' + aid + '.csv'\n",
    "    data = pd.read_csv(os.path.join(audio_folder, file), index_col=0).to_numpy()\n",
    "    \n",
    "    num_frames = data.shape[0]\n",
    "    if data.shape[0] < max_frames:\n",
    "        b_audio_data[i,:num_frames,:] = data\n",
    "    else:\n",
    "        b_audio_data[i,:,:] = data[:800,:]\n",
    "        \n",
    "print(b_audio_data.shape)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768800, 13)\n"
     ]
    }
   ],
   "source": [
    "d = audio_data.copy()\n",
    "d = d.reshape((d.shape[0]*d.shape[1], d.shape[2]))\n",
    "d = d[:,:13]\n",
    "\n",
    "df = pd.DataFrame(d, columns=[str(i) for i in range(13)])\n",
    "df.to_csv('audio_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of LSTM model\n",
    "class LSTMModel:\n",
    "    def __init__(self, data, labels, num_audio, max_frames, num_features):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.num_audio = num_audio\n",
    "        self.max_frames = max_frames\n",
    "        self.num_features = num_features\n",
    "        self.batch_size = 64\n",
    "        self.lstm_dim = 100\n",
    "        self.model = None\n",
    "        self.train_data = None\n",
    "        self.test_data = None\n",
    "        self.train_labels = None\n",
    "        self.test_labels = None\n",
    "        \n",
    "    def prepare_data(self, labels):\n",
    "        \n",
    "        # divide into train and test sets\n",
    "        self.train_data, self.test_data, self.train_labels, self.test_labels = \\\n",
    "            train_test_split(self.data, labels, test_size=0.3)\n",
    "\n",
    "        num_train, num_test = self.train_data.shape[0], self.test_data.shape[0]\n",
    "        \n",
    "        # apply z-score normalization\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        \n",
    "        self.train_data = self.train_data.reshape((num_train*self.max_frames, self.num_features))\n",
    "        self.train_data = scaler.fit_transform(self.train_data)\n",
    "        self.train_data = self.train_data.reshape((num_train, self.max_frames, self.num_features))\n",
    "        \n",
    "        self.test_data = self.test_data.reshape((num_test*self.max_frames, self.num_features))\n",
    "        self.test_data = scaler.transform(self.test_data)\n",
    "        self.test_data = self.test_data.reshape((num_test, self.max_frames, self.num_features))\n",
    "        \n",
    "        print(\"Train data size: \", self.train_data.shape, self.train_labels.shape)\n",
    "        print(\"Test data size: \", self.test_data.shape, self.test_labels.shape)\n",
    "        \n",
    "    def build_regression_model(self):\n",
    "        print(\"Building model ...\")\n",
    "        # construct regression model\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(self.batch_size, input_shape=(self.max_frames, self.num_features)))\n",
    "        self.model.add(Dense(300, activation='relu'))\n",
    "        self.model.add(Dense(150, activation='relu'))\n",
    "        self.model.add(LSTM(self.lstm_dim, return_sequences=False))\n",
    "        self.model.add(Dense(1, activation='linear'))\n",
    "\n",
    "        self.model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
    "        \n",
    "        print(self.model.summary())\n",
    "        plot_model(self.model, 'audio_regression_model.png', show_shapes=True)\n",
    "        \n",
    "    def train_regression_model(self):\n",
    "        \n",
    "        self.prepare_data(self.labels)\n",
    "\n",
    "        i = self.num_audio // self.batch_size\n",
    "        i *= self.batch_size\n",
    "        self.train_data = self.train_data[:i,:,:]\n",
    "        self.train_labels = self.train_labels[:i]\n",
    "\n",
    "        print(\"Training model ...\")\n",
    "        self.model.fit(self.train_data, self.train_labels, \n",
    "                  batch_size=self.batch_size, shuffle=True, epochs=7)\n",
    "        \n",
    "    def test_regression_model(self):\n",
    "        print(\"Predicting ...\")\n",
    "        self.model.evaluate(self.test_data, self.test_labels)\n",
    "        pred = self.model.predict(self.test_data)\n",
    "        pred = pred.squeeze()\n",
    "        self.test_labels = self.test_labels.tolist()\n",
    "        \n",
    "\n",
    "        error = [p-t for p, t in zip(pred, self.test_labels)]\n",
    "\n",
    "        error = pd.Series(error)\n",
    "        print(error.value_counts(bins=10).sort_index())\n",
    "        error.hist(bins=10)\n",
    "        \n",
    "        \"\"\"\n",
    "        sns.distplot(error, hist=False, rug=True, axlabel='error')\n",
    "        plt.show()\n",
    "        \"\"\"\n",
    "        \n",
    "    def build_classification_model(self):\n",
    "        print(\"Building model ...\")\n",
    "        # construct classification model\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(self.batch_size, input_shape=(self.max_frames, self.num_features)))\n",
    "        self.model.add(Dense(300, activation='relu'))\n",
    "        self.model.add(Dense(150, activation='relu'))\n",
    "        self.model.add(LSTM(self.lstm_dim, return_sequences=False))\n",
    "        self.model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        self.model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "        \n",
    "        print(self.model.summary())\n",
    "        plot_model(self.model, 'audio_classification_model.png', show_shapes=True)\n",
    "        \n",
    "    def train_classification_model(self):\n",
    "        # convert labels to 0 and 1\n",
    "        # 0 = not useful, 1 = useful\n",
    "        binary_labels = []\n",
    "        for l in self.labels:\n",
    "            if l < 0.5:\n",
    "                binary_labels.append(0)\n",
    "            else:\n",
    "                binary_labels.append(1)\n",
    "        encoder = LabelEncoder()\n",
    "        encoder.fit(binary_labels)\n",
    "        labels = encoder.transform(binary_labels)\n",
    "\n",
    "        self.prepare_data(labels)\n",
    "\n",
    "        i = self.num_audio // self.batch_size\n",
    "        i *= self.batch_size\n",
    "        self.train_data = self.train_data[:i,:,:]\n",
    "        self.train_labels = self.train_labels[:i]\n",
    "\n",
    "        print(\"Training model ...\")\n",
    "        self.model.fit(self.train_data, self.train_labels, \n",
    "                  batch_size=self.batch_size, shuffle=True, epochs=7)\n",
    "        \n",
    "    def test_classification_model(self):\n",
    "        print(\"Predicting ...\")\n",
    "        self.model.evaluate(self.test_data, self.test_labels)\n",
    "        pred = self.model.predict(self.test_data)\n",
    "        pred[pred < 0.5] = 0\n",
    "        pred[pred >= 0.5] = 1\n",
    "        \n",
    "        print(classification_report(self.test_labels, pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model ...\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 800, 64)           896       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 800, 300)          19500     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 800, 150)          45150     \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               100400    \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 166,047\n",
      "Trainable params: 166,047\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train data size:  (672, 800, 13) (672,)\n",
      "Test data size:  (289, 800, 13) (289,)\n",
      "Training model ...\n",
      "Train on 672 samples\n",
      "Epoch 1/7\n",
      "672/672 [==============================] - 11s 17ms/sample - loss: 0.1849 - mean_squared_error: 0.1849\n",
      "Epoch 2/7\n",
      "672/672 [==============================] - 11s 16ms/sample - loss: 0.1117 - mean_squared_error: 0.1117\n",
      "Epoch 3/7\n",
      "672/672 [==============================] - 11s 16ms/sample - loss: 0.0874 - mean_squared_error: 0.0874\n",
      "Epoch 4/7\n",
      "672/672 [==============================] - 11s 16ms/sample - loss: 0.0759 - mean_squared_error: 0.0759\n",
      "Epoch 5/7\n",
      "672/672 [==============================] - 11s 17ms/sample - loss: 0.0726 - mean_squared_error: 0.0726\n",
      "Epoch 6/7\n",
      "672/672 [==============================] - 11s 17ms/sample - loss: 0.0687 - mean_squared_error: 0.0687\n",
      "Epoch 7/7\n",
      "672/672 [==============================] - 11s 16ms/sample - loss: 0.0647 - mean_squared_error: 0.0647\n",
      "Predicting ...\n",
      "289/289 [==============================] - 2s 6ms/sample - loss: 0.0870 - mean_squared_error: 0.0870\n",
      "(-0.631, -0.502]     16\n",
      "(-0.502, -0.376]     18\n",
      "(-0.376, -0.25]      31\n",
      "(-0.25, -0.124]      46\n",
      "(-0.124, 0.00216]    35\n",
      "(0.00216, 0.128]     33\n",
      "(0.128, 0.254]       43\n",
      "(0.254, 0.38]        35\n",
      "(0.38, 0.507]        26\n",
      "(0.507, 0.633]        6\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mfcc_data = audio_data[:,:,:13]\n",
    "num_features = 13\n",
    "audio_model = LSTMModel(mfcc_data, audio_ratings, num_audio, max_frames, num_features)\n",
    "audio_model.build_regression_model()\n",
    "audio_model.train_regression_model()\n",
    "audio_model.test_regression_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model ...\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_20 (Dense)             (None, 800, 64)           896       \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 800, 300)          19500     \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 800, 150)          45150     \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 100)               100400    \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 166,047\n",
      "Trainable params: 166,047\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train data size:  (672, 800, 13) (672,)\n",
      "Test data size:  (289, 800, 13) (289,)\n",
      "Training model ...\n",
      "Train on 672 samples\n",
      "Epoch 1/7\n",
      "672/672 [==============================] - 12s 17ms/sample - loss: 0.6951 - acc: 0.4985\n",
      "Epoch 2/7\n",
      "672/672 [==============================] - 11s 16ms/sample - loss: 0.6764 - acc: 0.5893\n",
      "Epoch 3/7\n",
      "672/672 [==============================] - 11s 16ms/sample - loss: 0.6536 - acc: 0.6310\n",
      "Epoch 4/7\n",
      "672/672 [==============================] - 11s 16ms/sample - loss: 0.6295 - acc: 0.6339\n",
      "Epoch 5/7\n",
      "672/672 [==============================] - 11s 17ms/sample - loss: 0.5980 - acc: 0.6548\n",
      "Epoch 6/7\n",
      "672/672 [==============================] - 11s 16ms/sample - loss: 0.5625 - acc: 0.6890\n",
      "Epoch 7/7\n",
      "128/672 [====>.........................] - ETA: 10s - loss: 0.4878 - acc: 0.7266"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-01c78cb63e58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0maudio_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTMModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmfcc_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_ratings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_audio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0maudio_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_classification_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0maudio_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_classification_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0maudio_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_classification_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-ae9dae7a694c>\u001b[0m in \u001b[0;36mtrain_classification_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training model ...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         self.model.fit(self.train_data, self.train_labels, \n\u001b[0;32m--> 123\u001b[0;31m                   batch_size=self.batch_size, shuffle=True, epochs=7)\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest_classification_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mfcc_data = audio_data[:,:,:13]\n",
    "num_features = 13\n",
    "audio_model = LSTMModel(mfcc_data, audio_ratings, num_audio, max_frames, num_features)\n",
    "audio_model.build_classification_model()\n",
    "audio_model.train_classification_model()\n",
    "audio_model.test_classification_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
